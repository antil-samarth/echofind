# EchoFind: Music Recognition in Rust

[![Language: Rust](https://img.shields.io/badge/language-Rust-orange.svg)](https://www.rust-lang.org/)
<!-- [![Database: SQLite](https://img.shields.io/badge/database-SQLite-blue.svg)](https://sqlite.org/index.html) -->
<!-- Add other badges later, e.g., license, build status -->

A project implementing the core principles of audio fingerprinting and recognition, similar to Shazam or Google Pixel's "Now Playing" feature, built using Rust and SQLite.

## Motivation & Background

This project was inspired by the impressive "Now Playing" feature on my Google Pixel phone. Its ability to quickly and accurately identify songs playing in the background, seemingly offline and without needing long samples, sparked curiosity about the underlying technology.

The primary goals for building EchoFind are:

1.  **Explore Audio Fingerprinting:** To understand and implement the algorithms that make robust song recognition possible.
2.  **Showcase Language Versatility:** To dive into **Rust**, a modern, performant systems programming language known for its safety, speed, and growing ecosystem. This project serves as a practical application to learn and demonstrate proficiency in Rust.
3.  **Utilize Widely-Used Technologies:** To gain hands-on experience with **SQLite3**, a lightweight, embedded, and extensively used database engine, managing the storage and retrieval of audio fingerprints.

## Features (Current & Planned)

*   **Audio Loading:** Reads standard WAV audio files.
*   **Signal Processing:** Converts stereo to mono, downsamples audio for efficiency.
*   **Spectrogram Generation:** Computes the Short-Time Fourier Transform (STFT) using `rustfft` to analyze frequency content over time.
*   **Peak Finding:** Identifies prominent time-frequency peaks in the spectrogram, which serve as audio landmarks.
*   **Fingerprint Hashing:** Implements the anchor/target hashing strategy to create robust fingerprint hashes based on pairs of peaks.
    *   `(anchor_freq, target_freq, time_delta)` encoded into a compact `u64` hash.
*   **Visualization:** Generates images of the spectrogram and identified peaks using the `image` crate.
*   **TBD**
*   **Database Storage (Phase 2):** Storing generated fingerprints `(hash, time_offset, song_id)` in an SQLite database using `rusqlite`.
*   **Song Matching (Phase 3):** Comparing fingerprints from an audio snippet against the database using a time offset histogram method to identify the most likely match.

## Technology Stack

*   **Language:** [Rust](https://www.rust-lang.org/) (Stable toolchain)
*   **Database:** [SQLite](https://sqlite.org/index.html)
*   **Core Crates:**
    *   `hound`: For reading WAV audio files.
    *   `rustfft`: For Fast Fourier Transform calculations.
    *   `image`: For spectrogram visualization.
    *   `rusqlite`: For SQLite database interaction (to be added in Phase 2).

## Algorithm Overview

1.  **Preprocessing:** Audio is loaded, converted to mono, and downsampled (e.g., to 8192 Hz) to reduce computational load while retaining essential frequencies.
2.  **STFT:** The audio is divided into short, overlapping windows. The Fast Fourier Transform (FFT) is applied to each window (after applying a Hamming window function) to get the frequency spectrum for that short time segment. This creates a spectrogram (time vs. frequency vs. intensity).
3.  **Peak Finding:** For each time slice in the spectrogram, the algorithm identifies frequency peaks with the highest energy within predefined frequency bands. These `(time, frequency)` points form a "constellation map" of the most robust audio features.
4.  **Hashing:** Pairs of nearby peaks (an "anchor" peak and a subsequent "target" peak within a target zone) are combined. A hash value is generated encoding the anchor frequency, target frequency, and the time difference between them.
5.  **Storage (Phase 2):** Each generated `(hash, anchor_time_offset, song_id)` tuple is stored in an indexed SQLite database. (Phase 2 Target)
6.  **Matching (Phase 3):** Generate hashes for an unknown audio snippet and query the database. Identify candidate songs based on hash matches. The final identification relies on finding statistically significant alignment in the `time_offset` values between the snippet and database songs (using a histogram approach). (Phase 3 Target) 

### Database Setup (Phase 2 onwards)

## Project Structure (Simplified)

```
echofind/
├── mp3_to_wav.bat   # Script to convert MP3 audio files to WAV using `ffmpeg` made with the help of Claude (Windows)
├── Cargo.toml       # Project manifest and dependencies
├── README.md        # This file
├── src/
│   ├── main.rs      # Main application logic (entry point)
│   ├── media/       # Sample audio files, generated images
│   └── [other .rs files for modules - planned]
├── target/          # Build artifacts (generated by cargo)
└── [database.sqlite] # SQLite database file (generated on first run/add)
```

## Future Enhancements

*   Implement real-time microphone input using `cpal`.
*   Build the command-line interface using `clap`.
*   Implement database storage and retrieval (`rusqlite`).
*   Support for more audio formats (e.g., using `symphonia`).
*   Explore more advanced/robust peak finding algorithms.
*   Performance optimizations for fingerprinting and matching.
*   Add basic song metadata handling (reading tags, storing title/artist).